\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{color}
\usepackage{subcaption}
\usepackage{float}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4}    % how many sectioning levels to show in ToC
\begin{document}
\title{\textbf{CS6011: Kernel Methods for Pattern Analysis}
\\
\textbf{Term Project Initial Report \\[10pt]} 
\textbf{Fast Support Vector Data Descriptions
for Novelty Detection
}
}
\author{
Yi-Hung Liu, Yan-Chen Liu, and Yen-Jen Chen \\[10pt]
Aravind Sankar CS11B033 \\
Adit Krishnan CS11B063 \\[0.2in]
Team 14
}
\floatplacement{figure}{H}
\maketitle
\tableofcontents 
\newpage

\section{Brief Summary}
\subsection{Introduction}

In this paper, the authors address the common well-studied problem of Novelty Detection. In this task, we have a 2-class problem denoted by the normal and abnormal class points, where the number of points of abnormal class is lesser than that of the normal class. The Support Vector Data Description (SVDD) is an important and popular technique which has been used to solve the problem of Novelty Detection. In short, SVDD tries to construct a soft minimal hypersphere around the points of the target class (normal) in the kernel feature space ($\phi$). SVDD like any other SVM, has a prediction time complexity linear in the number of support vectors ($N_s$) because $N_s$ evaluations of the kernel function is required for predicting the label of an unseen data points. \\[10pt]

The authors attempt to address this key issue in this paper by proposing Fast SVDD (F-SVDD) which replaces the kernel expansion (of $N_s$ terms) in the decision function with just 1 term. This is achieved by first looking for a vector (called agent of the center) in the $\phi$-space whose preimage $\hat{x}$ exists in the $x$-space. Then, they express the sphere center $a_{\phi}$ as a scalar multiple of this vector in the $\phi$-space. Now, it can be seen the decision function involves only one kernel function between $\hat{x}$ and any unseen data point $x$. This means that the prediction time complexity is constant, irrespective of the size of the training dataset. The authors also propose an efficient method for solving the preimage problem, which is non-iterative and faster than the existing techniques. The way they solve these problems will be explained in the further sections.


\subsection{SVDD and some properties}
In this section, we introduce the SVDD problem definition for the purpose of notation. \\[10pt]

\textbf{Primal problem :}
\begin{equation}
\begin{split}
\text{min} \; R^2 + C \sum\limits_{i=1}^N \xi_i \\
\text{subject to}\\
||\phi(x_i) - a_{\phi}||^2  \leq R^2 + \xi_i \\
\xi_i \geq 0 \; \forall i = 1,...N
\end{split}
\end{equation}

\textbf{Dual problem:}

\begin{equation}
\begin{split}
\text{max}\; 1 - \sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j K(x_i,x_j) \\
\text{subject to}  \\
\sum\limits_{i = 1}^N \alpha_i  = 1  \\
0 \leq \alpha_i \leq C \;\; \forall  i = 1,...,N.  
\end{split}
\end{equation}


Here, the discriminant function $g(x)$ is given by :

\[ g(x) = ||\phi(x_i) - a_{\phi}||^2 - R^2 = c - 2 \sum\limits_{i=1}^{N_s}\alpha_i K(x,x_i) \]
where $c$ is a constant.

The sphere center $a_\phi$ is given by :

\[ a_\phi  = \sum\limits_{i=1}^{N} \alpha_i \phi(x_i) = \sum\limits_{i=1}^{N_s} \alpha_i \phi(x_i)\]

We also compute $||a_\phi||$ as it will be used later, 

\[||a_\phi||^2 = ||\sum\limits_{i=1}^{N_s} \alpha_i \phi(x_i) ||^2  = \alpha^T K \alpha\]
where $K$ is the kernel gram matrix.


Let's denote the soft minimal hypersphere as $B_S$. We assume the use of a normalized kernel function so that $K(x,x) = 1$. So, all points $\phi(x)$ lie on a unit hypersphere (centered at origin $O_\phi$) in the kernel feature space, denoted by $B_\phi$. This leads to interesting geometrical properties in the kernel feature space. One main property is mentioned below : \\[10pt]

\textbf{Property :} The center $a_\phi$ of the SVDD hypersphere $B_S$ must lie inside the unit hypersphere $B_F$.


This property shows that the exact preimage of $a_\phi$ in the x-space does not exist. This means that, either only an approximate preimage can be found, or a different way needs to be found. We show how this property is used in F-SVDD.

\subsection{Fast SVDD}
The main objective of F-SVDD is to improve classification speed by reducing the number of computations at prediction time. By the above property, the preimage of the center $a_\phi$ does not exist. This algorithm proceeds in 3 steps :
\begin{enumerate}
\item First we define the agent of the center $\Psi_a$ (in the kernel feature space).
\begin{itemize}

\item $\Psi_a$ is identified as the point closest to $a_\phi$ that lies on $B_F$. This point obviously is in the direction of $a_\phi$, as it's obtained by extending $a_\phi$ to intersect $B_F$.

\item Thus, we can write $\Psi_a = \gamma a_\phi$ where $\gamma$  is a scalar. Also, $\gamma >1$ as $||a_\phi|| < 1$.

\end{itemize}
\item Next, we find the preimage of the agent. 
\begin{itemize}

\item This is found by solving the problem :
\[ \hat{x} = \min_{\hat{x}} ||\Psi_a - \phi(\hat{x}) ||^2   \]

\item It can shown that solving this problem is equivalent to finding the approximate preimage of $a_\phi$, i.e.

\[ \min \rho = ||a_\phi - \phi(\hat{x}) ||^2 \]

\item On setting the derivative of the objective function ($\rho$) to 0, we get 

\[ \hat{x} = \frac{\sum\limits_{i=1}^N \alpha_i K(\hat{x},x_i)x_i}{\sum\limits_{i=1}^N K(\hat{x},x_i)} \]

\item Fixed point iteration is an existing technique which can be used here to solve for $\hat{x}$. This method faces the problems of local minima and multiple initial guesses for $\hat{x}$.
\end{itemize}
\item Next, a direct non-iterative preimage finding method is proposed.
\begin{itemize}
\item In this technique, some simple properties of kernel functions are used to arrive at a closed form expression for $\hat{x}$.
\[ \hat{x} = \frac{\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j K(x_i,x_j) x_i}{\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j K(x_i,x_j)} \]
\item From the above expression, we obtain a closed form expression for $\hat{x}$ which avoids the problems faced by the fixed point iteration method.
\end{itemize}
\item Finally, the discriminant function $g(x)$ is given by 

\begin{equation}
\begin{split}
g(x) &= || \phi(x) - a_\phi||^2 - R^2 = ||\phi(x) - \frac{\phi(\hat{x})}{\gamma} || - R^2 \\
 &=   c' - \frac{2}{\gamma} K(x,\hat{x}) \\
 & \text{where}\;  c' = 1- R^2 + 1/\gamma^2 \; \text{is a constant}.
\end{split}
\end{equation}
Thus, we can see that the $g(x)$ has only one kernel function term, which makes the prediction time complexity a constant.
\end{enumerate}

\section{Datasets to be used}

\begin{enumerate}
\item \href{https://archive.ics.uci.edu/ml/datasets/Iris}{Fisher Iris Dataset}

\item \href{https://archive.ics.uci.edu/ml/datasets/Wine}{Wine recognition Dataset}
\item \href{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)}{Breast Cancer Dataset}
\end{enumerate}


\section{Metrics for Evaluation}

\section{Algorithms to be implemented}

The F-SVDD algorithm will be implemented using both the fixed point iteration method (F-SVDD-1) and the direct preimage finding method. These results will be compared with the existing method of C-SVDD. The non-kernel method that will be used here, is the MLFFNN back propagation neural network. This comparison with MLFFNN is mainly to test the generalization ability of the proposed model F-SVDD.
\end{document}